{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Spark-NLP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bang2018/SPARK-ML/blob/main/Spark_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XtCa0sZ8EXdj"
      },
      "source": [
        "#**Introduction**\n",
        "\n",
        "###This tutorial explains the following:\n",
        "\n",
        "   ### 1. Installation of Spark NLP\n",
        "   ### 2. Classifier for 1.3 million sarcastic comments\n",
        "   ### 3. Text summarization using T5 Transformer\n",
        "\n",
        "###The dataset contains 1.3 million Sarcastic comments from the Internet commentary website Reddit. The data was gathered by: Mikhail Khodak and Nikunj Saunshi and Kiran Vodrahalli for their article \"A Large Self-Annotated Corpus for Sarcasm\". \n",
        "\n",
        "###**Contribution**: My classifier got 64% accuracy. John Snow LABS got 60% accuracy. \n",
        "                 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4Ffr_tofCW0"
      },
      "source": [
        "##Installation of Spark-NLP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tyMMD_upEfIa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b56bd7fd-6fd4-4f62-a7d8-9fedc6797659"
      },
      "source": [
        "!wget http://setup.johnsnowlabs.com/colab.sh -O - | bash"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-09-02 18:32:09--  http://setup.johnsnowlabs.com/colab.sh\n",
            "Resolving setup.johnsnowlabs.com (setup.johnsnowlabs.com)... 51.158.130.125\n",
            "Connecting to setup.johnsnowlabs.com (setup.johnsnowlabs.com)|51.158.130.125|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/scripts/colab_setup.sh [following]\n",
            "--2021-09-02 18:32:09--  https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/scripts/colab_setup.sh\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1608 (1.6K) [text/plain]\n",
            "Saving to: ‘STDOUT’\n",
            "\n",
            "-                   100%[===================>]   1.57K  --.-KB/s    in 0s      \n",
            "\n",
            "2021-09-02 18:32:10 (34.3 MB/s) - written to stdout [1608/1608]\n",
            "\n",
            "setup Colab for PySpark 3.1.2 and Spark NLP 3.2.2\n",
            "Hit:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease\n",
            "Ign:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:3 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Ign:4 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Hit:6 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Hit:8 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:10 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Hit:11 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Hit:13 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Get:14 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Hit:15 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Fetched 252 kB in 3s (97.3 kB/s)\n",
            "Reading package lists... Done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5in-TmzGEXdk",
        "outputId": "0243b552-e886-4f4b-9a11-4e1625d76296"
      },
      "source": [
        "import sparknlp\n",
        "spark = sparknlp.start()\n",
        "\n",
        "print(f\"Spark NLP version: {sparknlp.version()}\")\n",
        "print(f\"Apache Spark version: {spark.version}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spark NLP version: 3.2.2\n",
            "Apache Spark version: 3.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08IZxDPVfLjw"
      },
      "source": [
        "##Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JDUEfENExaLK",
        "outputId": "0b2144dc-7bd4-4e5d-8dd0-9f303f61b6e5"
      },
      "source": [
        "! wget -N https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/resources/en/sarcasm/train-balanced-sarcasm.csv -P /tmp"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-09-02 18:33:02--  https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/resources/en/sarcasm/train-balanced-sarcasm.csv\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.38.254\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.38.254|:443... connected.\n",
            "HTTP request sent, awaiting response... 304 Not Modified\n",
            "File ‘/tmp/train-balanced-sarcasm.csv’ not modified on server. Omitting download.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GX_Jcfikxezb"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName(\"ML SQL session\") \\\n",
        "    .config('spark.executor.instances','2') \\\n",
        "    .config(\"spark.executor.memory\", \"2g\") \\\n",
        "    .config(\"spark.driver.memory\",\"16g\") \\\n",
        "    .getOrCreate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DV4ZYQV_xwhX",
        "outputId": "264ecc09-ca77-4e7c-b91a-af0c5c46de43"
      },
      "source": [
        "from pyspark.sql import SQLContext\n",
        "sql = SQLContext(spark)\n",
        "trainDF = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(\"/tmp/train-balanced-sarcasm.csv\")\n",
        "trainDF.printSchema()\n",
        "#Creating View\n",
        "trainDF.createOrReplaceTempView('Sarcasm')\n",
        "sql.sql('SELECT COUNT(*) FROM Sarcasm').collect()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- label: integer (nullable = true)\n",
            " |-- comment: string (nullable = true)\n",
            " |-- author: string (nullable = true)\n",
            " |-- subreddit: string (nullable = true)\n",
            " |-- score: string (nullable = true)\n",
            " |-- ups: string (nullable = true)\n",
            " |-- downs: string (nullable = true)\n",
            " |-- date: string (nullable = true)\n",
            " |-- created_utc: string (nullable = true)\n",
            " |-- parent_comment: string (nullable = true)\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(count(1)=1010826)]"
            ]
          },
          "metadata": {},
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KY4-f1A8WXQY",
        "outputId": "c82c2a49-582f-41d9-ae75-8f0dc054931b"
      },
      "source": [
        "#Check the datatype\n",
        "type(trainDF)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pyspark.sql.dataframe.DataFrame"
            ]
          },
          "metadata": {},
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UNyu2-8Px5Ms",
        "outputId": "5bcc98a7-5a26-43a3-e326-558ae60a02f5"
      },
      "source": [
        "sql.sql('select * from Sarcasm where author=\"Trumpbart\"').show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+----------------+---------+---------+-----+---+-----+-------+-------------------+--------------------+\n",
            "|label|         comment|   author|subreddit|score|ups|downs|   date|        created_utc|      parent_comment|\n",
            "+-----+----------------+---------+---------+-----+---+-----+-------+-------------------+--------------------+\n",
            "|    0|      NC and NH.|Trumpbart| politics|    2| -1|   -1|2016-10|2016-10-16 23:55:23|Yeah, I get that ...|\n",
            "|    1|Very surprising!|Trumpbart| politics|    1| -1|   -1|2016-10|2016-10-26 04:35:59|t_d is defending ...|\n",
            "+-----+----------------+---------+---------+-----+---+-----+-------+-------------------+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "db54ARjvx_HI",
        "outputId": "07acff7d-02b6-46d4-ba93-aedd91320354"
      },
      "source": [
        "sql.sql('select label,count(1) as label_count from Sarcasm group by label order by label_count desc').show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----------+\n",
            "|label|label_count|\n",
            "+-----+-----------+\n",
            "|    1|     505413|\n",
            "|    0|     505413|\n",
            "+-----+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "junSZdfWyEhQ",
        "outputId": "ef438329-9712-45fa-fc8a-f515a47f02fe"
      },
      "source": [
        "df = sql.sql('select label,concat(parent_comment,\"|\",comment) as comment from Sarcasm where comment is not null and parent_comment is not null limit 10000')\n",
        "df.printSchema()\n",
        "df.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- label: integer (nullable = true)\n",
            " |-- comment: string (nullable = true)\n",
            "\n",
            "+-----+--------------------+\n",
            "|label|             comment|\n",
            "+-----+--------------------+\n",
            "|    0|Yeah, I get that ...|\n",
            "|    0|The blazers and M...|\n",
            "|    0|They're favored t...|\n",
            "|    0|deadass don't kil...|\n",
            "|    0|Yep can confirm I...|\n",
            "|    0|do you find arian...|\n",
            "|    0|What's your weird...|\n",
            "|    0|Probably Sephirot...|\n",
            "|    0|What to upgrade? ...|\n",
            "|    0|Probably count Ka...|\n",
            "|    0|I bet if that mon...|\n",
            "|    0|James Shields Wil...|\n",
            "|    0|There's no time t...|\n",
            "|    0|Team Specific Thr...|\n",
            "|    0|Ill give you a hi...|\n",
            "|    0|Star Wars, easy. ...|\n",
            "|    0|You're adorable.|...|\n",
            "|    0|He actually acts ...|\n",
            "|    0|Clinton struggles...|\n",
            "|    0|Is that the Older...|\n",
            "+-----+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ru_-POUTyKbc",
        "outputId": "98ce1589-16a3-4a52-a779-2a0c81316da1"
      },
      "source": [
        "from sparknlp.annotator import *\n",
        "from sparknlp.common import *\n",
        "from sparknlp.base import *\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "\n",
        "docs_assembler = DocumentAssembler() \\\n",
        "    .setInputCol(\"comment\") \\\n",
        "    .setOutputCol(\"document\")\n",
        "    \n",
        "sent_detector = SentenceDetector() \\\n",
        "    .setInputCols([\"document\"]) \\\n",
        "    .setOutputCol(\"sentence\") \\\n",
        "    .setUseAbbreviations(True)\n",
        "    \n",
        "token = Tokenizer() \\\n",
        "  .setInputCols([\"sentence\"]) \\\n",
        "  .setOutputCol(\"token\")\n",
        "\n",
        "stem = Stemmer() \\\n",
        "    .setInputCols([\"token\"]) \\\n",
        "    .setOutputCol(\"stem\")\n",
        "    \n",
        "norm = Normalizer() \\\n",
        "    .setInputCols([\"stem\"]) \\\n",
        "    .setOutputCol(\"normalized\")\n",
        "\n",
        "result = Finisher() \\\n",
        "    .setInputCols([\"normalized\"]) \\\n",
        "    .setOutputCols([\"ntokens\"]) \\\n",
        "    .setOutputAsArray(True) \\\n",
        "    .setCleanAnnotations(True)\n",
        "\n",
        "nlp_pipe = Pipeline(stages=[docs_assembler, sent_detector, token, stem, norm,result])\n",
        "nlp_model = nlp_pipe.fit(df)\n",
        "preprocessing = nlp_model.transform(df).persist()\n",
        "preprocessing.count()\n",
        "preprocessing.show()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+--------------------+--------------------+\n",
            "|label|             comment|             ntokens|\n",
            "+-----+--------------------+--------------------+\n",
            "|    0|Yeah, I get that ...|[yeah, i, get, th...|\n",
            "|    0|The blazers and M...|[the, blazer, and...|\n",
            "|    0|They're favored t...|[theyr, favor, to...|\n",
            "|    0|deadass don't kil...|[deadass, dont, k...|\n",
            "|    0|Yep can confirm I...|[yep, can, confir...|\n",
            "|    0|do you find arian...|[do, you, find, a...|\n",
            "|    0|What's your weird...|[what, your, weir...|\n",
            "|    0|Probably Sephirot...|[probabl, sephiro...|\n",
            "|    0|What to upgrade? ...|[what, to, upgrad...|\n",
            "|    0|Probably count Ka...|[probabl, count, ...|\n",
            "|    0|I bet if that mon...|[i, bet, if, that...|\n",
            "|    0|James Shields Wil...|[jame, shield, wi...|\n",
            "|    0|There's no time t...|[there, no, time,...|\n",
            "|    0|Team Specific Thr...|[team, specif, th...|\n",
            "|    0|Ill give you a hi...|[ill, give, you, ...|\n",
            "|    0|Star Wars, easy. ...|[star, war, easi,...|\n",
            "|    0|You're adorable.|...|  [your, ador, note]|\n",
            "|    0|He actually acts ...|[he, actual, act,...|\n",
            "|    0|Clinton struggles...|[clinton, struggl...|\n",
            "|    0|Is that the Older...|[i, that, the, ol...|\n",
            "+-----+--------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kY9sUTcVy4N9",
        "outputId": "7e731deb-b47f-4f05-fd2d-92175a558719"
      },
      "source": [
        "train, test = preprocessing.randomSplit(weights=[0.7, 0.3], seed=123)\n",
        "print(f\"Length of train dataset {train.count()}\")\n",
        "print(f\"Length of test dataset {test.count()}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of train dataset 6934\n",
            "Length of test dataset 3066\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rs9Maulzy-L0",
        "outputId": "7e5fd28c-fe50-49a9-8134-29eb27f91522"
      },
      "source": [
        "from pyspark.ml import feature as spark_ft\n",
        "\n",
        "stopWords = spark_ft.StopWordsRemover.loadDefaultStopWords('english')\n",
        "sw_remover = spark_ft.StopWordsRemover(inputCol='ntokens', outputCol='clean_tokens', stopWords=stopWords)\n",
        "tf = spark_ft.CountVectorizer(vocabSize=500, inputCol='clean_tokens', outputCol='tf')\n",
        "idf = spark_ft.IDF(minDocFreq=5, inputCol='tf', outputCol='idf')\n",
        "\n",
        "feature_pipeline = Pipeline(stages=[sw_remover, tf, idf])\n",
        "feature_model = feature_pipeline.fit(train)\n",
        "\n",
        "train_featurized = feature_model.transform(train).persist()\n",
        "train_featurized.count()\n",
        "train_featurized.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|label|             comment|             ntokens|        clean_tokens|                  tf|                 idf|\n",
            "+-----+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|    0|\"\"\"...newtons.\"\" ...|[newton, which, i...|[newton, dont, ge...|(500,[0,6,165],[1...|(500,[0,6,165],[1...|\n",
            "|    0|\"\"\"Agreed. I thin...|[agr, i, think, w...|[agr, think, issu...|(500,[0,1,7,13,31...|(500,[0,1,7,13,31...|\n",
            "|    0|\"\"\"And an underag...|[and, an, underag...|[underag, girlthi...|         (500,[],[])|         (500,[],[])|\n",
            "|    0|\"\"\"As an excuse\"\"...|[a, an, excus, so...|[excus, sorri, bu...|(500,[18,163,266]...|(500,[18,163,266]...|\n",
            "|    0|\"\"\"Budaya Ketimur...|[budaya, ketimura...|[budaya, ketimura...|(500,[0,1,7,35,15...|(500,[0,1,7,35,15...|\n",
            "|    0|\"\"\"Dear so called...|[dear, so, call, ...|[dear, call, gran...|    (500,[71],[2.0])|(500,[71],[7.2371...|\n",
            "|    0|\"\"\"Define Capital...|[defin, capit, bl...|[defin, capit, bl...|(500,[4,7,82,108,...|(500,[4,7,82,108,...|\n",
            "|    0|\"\"\"Exactly. My ki...|[exactli, my, kid...|[exactli, kid, pe...|(500,[2,12,18,163...|(500,[2,12,18,163...|\n",
            "|    0|\"\"\"Gingrich|And C...|[gingrichand, chr...|[gingrichand, chr...|(500,[16,107,444]...|(500,[16,107,444]...|\n",
            "|    0|\"\"\"Hmm|Moonkin ar...|[hmmmoonkin, ar, ...|[hmmmoonkin, ar, ...|     (500,[1],[1.0])|(500,[1],[1.89352...|\n",
            "|    0|\"\"\"Ho\"\" Speak lik...|[ho, speak, like,...|[ho, speak, like,...|(500,[4,31],[1.0,...|(500,[4,31],[1.98...|\n",
            "|    0|\"\"\"I don't know i...|[i, dont, know, i...|[dont, know, univ...|(500,[6,17,42,148...|(500,[6,17,42,148...|\n",
            "|    0|\"\"\"I think you ar...|[i, think, you, a...|[think, ar, overd...|(500,[1,13,40,42,...|(500,[1,13,40,42,...|\n",
            "|    0|\"\"\"I'm a man|Boja...|[im, a, manbojack...|[im, manbojack, h...|     (500,[9],[1.0])|(500,[9],[2.53623...|\n",
            "|    0|\"\"\"It's kind of h...|[it, kind, of, ha...|[kind, hard, turn...|(500,[6,139,155,1...|(500,[6,139,155,1...|\n",
            "|    0|\"\"\"John Sousa|App...|[john, sousaappar...|[john, sousaappar...|         (500,[],[])|         (500,[],[])|\n",
            "|    0|\"\"\"Legacy Python\"...|[legaci, pythonak...|[legaci, pythonak...|         (500,[],[])|         (500,[],[])|\n",
            "|    0|\"\"\"Make her mindg...|[make, her, mindg...|[make, mindgasm, ...|(500,[11,34],[1.0...|(500,[11,34],[2.5...|\n",
            "|    0|\"\"\"Mighty Mage\"\" ...|[mighti, mage, ha...|[mighti, mage, ha...|(500,[12,14,62,88...|(500,[12,14,62,88...|\n",
            "|    0|\"\"\"Mom|Holy shitb...|[momholi, shitbal...|[momholi, shitbal...|         (500,[],[])|         (500,[],[])|\n",
            "+-----+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "agpvkLzVzNT2",
        "outputId": "35e480d3-7bfa-4780-b4af-c570c7c66f33"
      },
      "source": [
        "train_featurized.groupBy(\"label\").count().show()\n",
        "train_featurized.printSchema()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----+\n",
            "|label|count|\n",
            "+-----+-----+\n",
            "|    0| 4357|\n",
            "|    1| 2577|\n",
            "+-----+-----+\n",
            "\n",
            "root\n",
            " |-- label: integer (nullable = true)\n",
            " |-- comment: string (nullable = true)\n",
            " |-- ntokens: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            " |-- clean_tokens: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            " |-- tf: vector (nullable = true)\n",
            " |-- idf: vector (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hBczGuyxzUCM"
      },
      "source": [
        "from pyspark.ml import classification as spark_cls\n",
        "rf = spark_cls.GBTClassifier(labelCol=\"label\", featuresCol=\"idf\")\n",
        "model = rf.fit(train_featurized)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wA7POsehzdtB",
        "outputId": "4b94cfbf-29e6-4ff3-865a-7046424544a1"
      },
      "source": [
        "test_featurized = feature_model.transform(test)\n",
        "preds = model.transform(test_featurized)\n",
        "preds.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
            "|label|             comment|             ntokens|        clean_tokens|                  tf|                 idf|       rawPrediction|         probability|prediction|\n",
            "+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
            "|    0|\"\"\"An unmet playe...|[an, unmet, playe...|[unmet, player, h...|(500,[0,1,7,9,12,...|(500,[0,1,7,9,12,...|[0.29389294149927...|[0.64285696333844...|       0.0|\n",
            "|    0|\"\"\"Christ Enthusi...|[christ, enthusia...|[christ, enthusia...|(500,[3,7,46,56,5...|(500,[3,7,46,56,5...|[0.25503848824496...|[0.62482451938123...|       0.0|\n",
            "|    0|\"\"\"Did Hillary Cl...|[did, hillari, cl...|[hillari, clinton...|(500,[19,106,198,...|(500,[19,106,198,...|[0.26936459449257...|[0.63151674521756...|       0.0|\n",
            "|    0|\"\"\"He's not norma...|[he, not, normal,...|[normal, typewher...|(500,[0,211,372,4...|(500,[0,211,372,4...|[0.17418836210872...|[0.58622388349987...|       0.0|\n",
            "|    0|\"\"\"Hey you wanna ...|[hei, you, wanna,...|[hei, wanna, get,...|(500,[5,6,10,17,6...|(500,[5,6,10,17,6...|[0.35275204077428...|[0.66940696619385...|       0.0|\n",
            "|    0|\"\"\"I'll buy almos...|[ill, bui, almost...|[ill, bui, almost...|(500,[73,97,104,1...|(500,[73,97,104,1...|[0.40567222069190...|[0.69239592259190...|       0.0|\n",
            "|    0|\"\"\"Lov\"\"\"|\"There'...|[lovthere, an, e,...|[lovthere, e, har...|   (500,[319],[1.0])|(500,[319],[4.766...|[0.35275204077428...|[0.66940696619385...|       0.0|\n",
            "|    0|\"\"\"People\"\"|Umm, ...|[peopleumm, he, c...|[peopleumm, cant,...|    (500,[41],[1.0])|(500,[41],[3.2059...|[0.35275204077428...|[0.66940696619385...|       0.0|\n",
            "|    0|\"\"\"Play it cool; ...|[plai, it, cool, ...|[plai, cool, plai...|(500,[25,51,60,24...|(500,[25,51,60,24...|[0.35275204077428...|[0.66940696619385...|       0.0|\n",
            "|    0|\"\"\"Said it last y...|[said, it, last, ...|[said, last, year...|(500,[1,3,22,23,2...|(500,[1,3,22,23,2...|[0.19948079969859...|[0.59843814766229...|       0.0|\n",
            "|    0|\"\"\"She didn't do ...|[she, didnt, do, ...|[didnt, anyth, he...|(500,[28,31,56,63...|(500,[28,31,56,63...|[0.35275204077428...|[0.66940696619385...|       0.0|\n",
            "|    0|\"\"\"Suspicous beha...|[suspic, behaviou...|[suspic, behaviou...|(500,[6,7,52,53,4...|(500,[6,7,52,53,4...|[0.23686548455416...|[0.61626644572649...|       0.0|\n",
            "|    0|\"\"\"The government...|[the, govern, i, ...|[govern, enslav, ...|(500,[164,391,469...|(500,[164,391,469...|[0.13137749975509...|[0.56531341070043...|       0.0|\n",
            "|    0|\"\"\"They claim the...|[thei, claim, the...|[thei, claim, evi...|(500,[2,3,15,148,...|(500,[2,3,15,148,...|[-0.0368257978804...|[0.48159542003297...|       1.0|\n",
            "|    0|\"\"\"Weird|I rememb...|[weirdi, rememb, ...|[weirdi, rememb, ...|(500,[30,70,122,1...|(500,[30,70,122,1...|[0.35275204077428...|[0.66940696619385...|       0.0|\n",
            "|    0|\"\"\"What the fuck ...|[what, the, fuck,...|[fuck, rooos, im,...|(500,[2,9,16,34,3...|(500,[2,9,16,34,3...|[0.36985868081600...|[0.67693404791776...|       0.0|\n",
            "|    0|\"(A turbo origina...|[a, turbo, origin...|[turbo, origin, b...|(500,[2,3,22,43,7...|(500,[2,3,22,43,7...|[0.17418836210872...|[0.58622388349987...|       0.0|\n",
            "|    0|\"*Guilt roles ove...|[guilt, role, ove...|[guilt, role, mel...|(500,[16,59,74,22...|(500,[16,59,74,22...|[0.35275204077428...|[0.66940696619385...|       0.0|\n",
            "|    0|\".....That's how ...|[that, how, you, ...|[read, read, orig...|(500,[4,31,39,49,...|(500,[4,31,39,49,...|[0.11039743345912...|[0.55497555806959...|       0.0|\n",
            "|    0|\"10 Good \"\"Horror...|[good, horror, mo...|[good, horror, mo...|(500,[23,25,49,56...|(500,[23,25,49,56...|[0.35275204077428...|[0.66940696619385...|       0.0|\n",
            "+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_RXkaOasasRF",
        "outputId": "991326c7-dd15-44d3-9333-537f2cd0ddc7"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "import numpy\n",
        "preds = preds.toPandas()\n",
        "result = accuracy_score(preds[\"label\"],preds[\"prediction\"])\n",
        "print(f\"Accuracy of the Classifier {round(result*100,2)}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the Classifier 64.42\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fEDKPL9N22s"
      },
      "source": [
        "#Text-to-Text Transfer Transformer(T5)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "di1LlQ3aOZ1m"
      },
      "source": [
        "###Text Summarization using T5 Transformer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eXihKsJ6N2B3",
        "outputId": "2f2fd319-66b6-45ff-a074-f7ffabc40f2c"
      },
      "source": [
        "docs_assemblers = DocumentAssembler()\\\n",
        "                  .setInputCol(\"text\")\\\n",
        "                  .setOutputCol(\"documents\")\n",
        "\n",
        "t5_transformer = T5Transformer()\\\n",
        "                 .pretrained(\"t5_small\",\"en\")\\\n",
        "                 .setTask(\"summarize\")\\\n",
        "                 .setMaxOutputLength(1000)\\\n",
        "                 .setInputCols([\"documents\"])\\\n",
        "                 .setOutputCol(\"summaries\")\n",
        "\n",
        "result = Pipeline(stages=[docs_assemblers,t5_transformer])\n",
        "                    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "t5_small download started this may take some time.\n",
            "Approximate size to download 139 MB\n",
            "[OK!]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8auJKgX_eej8",
        "outputId": "138f8407-314d-45d3-b819-f8dc7e717e1d"
      },
      "source": [
        "df = spark.createDataFrame([[\"\"]]).toDF(\"text\")\n",
        "df.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+\n",
            "|text|\n",
            "+----+\n",
            "|    |\n",
            "+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZ71L3pKew8H"
      },
      "source": [
        "model = result.fit(df)\n",
        "model2 = LightPipeline(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zBPMQ5DSfPrU"
      },
      "source": [
        "wiki_corpora_AI = \"\"\"\n",
        "Artificial intelligence (AI) is intelligence demonstrated by machines, as opposed to the natural intelligence displayed by humans or animals. Leading AI textbooks define the field as the study of \"intelligent agents\": any system that perceives its environment and takes actions that maximize its chance of achieving its goals.Some popular accounts use the term \"artificial intelligence\" to describe machines that mimic \"cognitive\" functions that humans associate with the human mind, such as \"learning\" and \"problem solving\", however this definition is rejected by major AI researchers.\n",
        "\n",
        "AI applications include advanced web search engines, recommendation systems (used by YouTube, Amazon and Netflix), understanding human speech (such as Siri or Alexa), self-driving cars (e.g. Tesla), and competing at the highest level in strategic game systems (such as chess and Go),[2] As machines become increasingly capable, tasks considered to require \"intelligence\" are often removed from the definition of AI, a phenomenon known as the AI effect. For instance, optical character recognition is frequently excluded from things considered to be AI, having become a routine technology.\n",
        "\n",
        "Artificial intelligence was founded as an academic discipline in 1956, and in the years since has experienced several waves of optimism, followed by disappointment and the loss of funding (known as an \"AI winter\"), followed by new approaches, success and renewed funding.[7][10] AI research has tried and discarded many different approaches during its lifetime, including simulating the brain, modeling human problem solving, formal logic, large databases of knowledge and imitating animal behavior. In the first decades of the 21st century, highly mathematical statistical machine learning has dominated the field, and this technique has proved highly successful, helping to solve many challenging problems throughout industry and academia.\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hst6_L6vgMOr",
        "outputId": "ff4c48c5-4ac0-4408-d74c-7df3dea5e0c3"
      },
      "source": [
        "result = model2.fullAnnotate(wiki_corpora_AI)[0]\n",
        "result"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'documents': [Annotation(document, 0, 1920, \n",
              "  Artificial intelligence (AI) is intelligence demonstrated by machines, as opposed to the natural intelligence displayed by humans or animals. Leading AI textbooks define the field as the study of \"intelligent agents\": any system that perceives its environment and takes actions that maximize its chance of achieving its goals.Some popular accounts use the term \"artificial intelligence\" to describe machines that mimic \"cognitive\" functions that humans associate with the human mind, such as \"learning\" and \"problem solving\", however this definition is rejected by major AI researchers.\n",
              "  \n",
              "  AI applications include advanced web search engines, recommendation systems (used by YouTube, Amazon and Netflix), understanding human speech (such as Siri or Alexa), self-driving cars (e.g. Tesla), and competing at the highest level in strategic game systems (such as chess and Go),[2] As machines become increasingly capable, tasks considered to require \"intelligence\" are often removed from the definition of AI, a phenomenon known as the AI effect. For instance, optical character recognition is frequently excluded from things considered to be AI, having become a routine technology.\n",
              "  \n",
              "  Artificial intelligence was founded as an academic discipline in 1956, and in the years since has experienced several waves of optimism, followed by disappointment and the loss of funding (known as an \"AI winter\"), followed by new approaches, success and renewed funding.[7][10] AI research has tried and discarded many different approaches during its lifetime, including simulating the brain, modeling human problem solving, formal logic, large databases of knowledge and imitating animal behavior. In the first decades of the 21st century, highly mathematical statistical machine learning has dominated the field, and this technique has proved highly successful, helping to solve many challenging problems throughout industry and academia.\n",
              "  , {})],\n",
              " 'summaries': [Annotation(document, 0, 273, leading AI textbooks define the field as the study of \"intelligent agents\" the term \"artificial intelligence\" is used to describe machines that mimic \"cognitive\" functions that humans associate with the human mind . the definition of AI is rejected by major AI researchers ., {})]}"
            ]
          },
          "metadata": {},
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xAba9US4hQsd",
        "outputId": "b923e2d2-b093-4962-ac15-7ef5d242b80c"
      },
      "source": [
        "print(\"Summary :\",result[\"summaries\"][0].result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary : leading AI textbooks define the field as the study of \"intelligent agents\" the term \"artificial intelligence\" is used to describe machines that mimic \"cognitive\" functions that humans associate with the human mind . the definition of AI is rejected by major AI researchers .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ECw6VstSMigm"
      },
      "source": [
        "## Text Summarization using Summa"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h8M39s0SMMPT",
        "outputId": "82bbc046-76d0-4010-d0af-f476a6473275"
      },
      "source": [
        "!pip install summa"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting summa\n",
            "  Downloading summa-1.2.0.tar.gz (54 kB)\n",
            "\u001b[?25l\r\u001b[K     |██████                          | 10 kB 18.1 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 20 kB 21.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 30 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 40 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 51 kB 3.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 54 kB 1.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=0.19 in /usr/local/lib/python3.7/dist-packages (from summa) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scipy>=0.19->summa) (1.19.5)\n",
            "Building wheels for collected packages: summa\n",
            "  Building wheel for summa (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for summa: filename=summa-1.2.0-py3-none-any.whl size=54410 sha256=14de373437f96576ae1a564727330a149e386eabeba6291b6d7535f7abfc50d4\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/64/ac/7b443477588d365ef37ada30d456bdf5f07dc5be9f6324cb6e\n",
            "Successfully built summa\n",
            "Installing collected packages: summa\n",
            "Successfully installed summa-1.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5_BnE8bLM_ho",
        "outputId": "242349fb-21bc-4b7b-8514-7dcc605006b8"
      },
      "source": [
        "from summa.summarizer import summarize\n",
        "print(summarize(wiki_corpora_AI, ratio=0.3))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Leading AI textbooks define the field as the study of \"intelligent agents\": any system that perceives its environment and takes actions that maximize its chance of achieving its goals.Some popular accounts use the term \"artificial intelligence\" to describe machines that mimic \"cognitive\" functions that humans associate with the human mind, such as \"learning\" and \"problem solving\", however this definition is rejected by major AI researchers.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "McfU44cUb5Yl"
      },
      "source": [
        "##References:\n",
        "\n",
        "1. https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/streamlit_notebooks/T5TRANSFORMER.ipynb\n",
        "\n",
        "2. https://towardsdatascience.com/hands-on-googles-text-to-text-transfer-transformer-t5-with-spark-nlp-6f7db75cecff\n",
        "\n",
        "3. https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Public/10.T5_Workshop_with_Spark_NLP.ipynb\n",
        "\n",
        "4. https://en.wikipedia.org/wiki/Artificial_intelligence\n",
        "\n"
      ]
    }
  ]
}